{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import textattack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本增强的使用示例\n",
    "\n",
    "重点展示下使用示例和生成速度, 例子都会用中文, **因为中文很可能不生效, 需要检查下.**\n",
    "\n",
    "\n",
    "TextAttack 的组件中，有很多易用的数据增强工具。textattack.Augmenter 类使用 变换 与一系列的 约束 进行数据增强。我们提供了 5 中内置的数据增强策略：\n",
    "\n",
    "- wordnet 通过基于 WordNet 同义词替换的方式增强文本\n",
    "- embedding 通过邻近词替换的方式增强文本，使用 counter-fitted 词嵌入空间中的邻近词进行替换，约束二者的 cosine 相似度不低于 0.8\n",
    "- charswap 通过字符的增删改，以及临近字符交换的方式增强文本\n",
    "- eda 通过对词的增删改来增强文本\n",
    "- checklist 通过简写，扩写以及对实体、地点、数字的替换来增强文本\n",
    "- clare 使用 pre-trained masked language model, 通过对词的增删改来增强文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不能创造我不理解的事物\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 主要的使用方式是两种\n",
    "# 第一种是使用已有的文本增强类\n",
    "from textattack.augmentation import EmbeddingAugmenter\n",
    "augmenter = EmbeddingAugmenter()\n",
    "s = 'What I cannot create, I do not understand.'\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不能创造我不理解的事1物\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 第二种是自定义转换器和约束, 构成新的文本增强器\n",
    "from textattack.transformations import WordSwapRandomCharacterDeletion\n",
    "from textattack.transformations import CompositeTransformation\n",
    "from textattack.augmentation import Augmenter\n",
    "transformation = CompositeTransformation([WordSwapRandomCharacterDeletion()])\n",
    "augmenter = Augmenter(transformation=transformation, transformations_per_example=5)\n",
    "s = 'What I cannot create, I do not understand.'\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有的类都是继承自 Augmenter. Augmenter 有几个实用的初始化参数\n",
    "\n",
    "1. pct_words_to_swap: 用于指定每个句子中需要增强的词的比例\n",
    "2. transformations_per_example: 每个句子中生成的增强的样本数量\n",
    "3. advanced_metrics: 返回高级指标, 包括 perplexity 和 USE Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 走吧, 带你扫除一切黑暗, by 妖刀姬"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回译\n",
    "\n",
    "原理: 将文本翻译成另一种语言, 再翻译回来, 生成新的文本."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.augmentation import BackTranslationAugmenter\n",
    "\n",
    "# 这个预制的类是为英文用的, 所以还得自己重新构建\n",
    "augmenter = BackTranslationAugmenter()\n",
    "s = \"What I cannot create, I do not understand.\"\n",
    "# s = \"我不能创造我不理解的事物\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不能创造我不明白的东西\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from textattack.transformations.sentence_transformations import BackTranslation\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.augmentation import Augmenter\n",
    "\n",
    "transformation = BackTranslation(\n",
    "    src_lang=\"zh\",\n",
    "    target_lang=\"en\",\n",
    "    src_model=\"Helsinki-NLP/opus-mt-en-zh\",\n",
    "    target_model=\"Helsinki-NLP/opus-mt-zh-en\",\n",
    ")\n",
    "constraints = [RepeatModification(), StopwordModification()]\n",
    "augmenter = Augmenter(transformation = transformation, constraints = constraints, transformations_per_example=1)\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<AttackedText \"我不能创造我不明白的东西\">], [<AttackedText \"我来自中国\">]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 试一下批量调用 BackTranslation\n",
    "from textattack.transformations.sentence_transformations import BackTranslation\n",
    "from textattack.shared import AttackedText\n",
    "\n",
    "transformation = BackTranslation(\n",
    "    src_lang=\"zh\",\n",
    "    target_lang=\"en\",\n",
    "    src_model=\"Helsinki-NLP/opus-mt-en-zh\",\n",
    "    target_model=\"Helsinki-NLP/opus-mt-zh-en\",\n",
    ")\n",
    "\n",
    "text_list = [\"我不能创造我不理解的事物\", \"我来自美丽中国\"]\n",
    "attacked_text_list = [AttackedText(text) for text in text_list]\n",
    "transformation.batch_call(attacked_text_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAREAugmenter\n",
    "\n",
    "原理: 使用 pre-trained masked language model, 通过对词的增删改来增强文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-08 13:08:02,980 SequenceTagger predicts: Dictionary with 19 tags: <unk>, NOUN, VERB, PUNCT, ADP, DET, PROPN, PRON, ADJ, ADV, CCONJ, PART, NUM, AUX, INTJ, SYM, X, <START>, <STOP>\n",
      "我不能创造出不理智的事物\n",
      "False\n",
      "我隻能編造我不理解的事物\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import CLAREAugmenter\n",
    "\n",
    "augmenter = CLAREAugmenter(\n",
    "    model=\"bert-base-chinese\",\n",
    "    tokenizer=\"bert-base-chinese\",\n",
    "    pct_words_to_swap=0.2,\n",
    "    transformations_per_example=2,\n",
    "    max_length=64,\n",
    ")\n",
    "\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字符交换\n",
    "\n",
    "对中文不生效, 因为这是对单词里的字符生效的, 我在前面的分词中是将每个中文字符当作一个单词的\n",
    "\n",
    "- (1) Swap: Swap two adjacent letters in the word.\n",
    "    - WordSwapNeighboringCharacterSwap(),\n",
    "- (2) Substitution: Substitute a letter in the word with a random letter.\n",
    "    - WordSwapRandomCharacterSubstitution(),\n",
    "- (3) Deletion: Delete a random letter from the word.\n",
    "    - WordSwapRandomCharacterDeletion(),\n",
    "- (4) Insertion: Insert a random letter in the word.\n",
    "    - WordSwapRandomCharacterInsertion(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不能创造我不理解的事物\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import CharSwapAugmenter\n",
    "augmenter = CharSwapAugmenter()\n",
    "\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "# s = \"What I cannot create, I do not understand.\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CheckListAugmenter\n",
    "\n",
    "Augments words by using the transformation methods provided by CheckList INV testing, which combines:\n",
    "\n",
    "- Name Replacement\n",
    "- Location Replacement\n",
    "- Number Alteration\n",
    "- Contraction/Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不能创造我不理解的事物\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import CheckListAugmenter\n",
    "\n",
    "augmenter = CheckListAugmenter()\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不能创造我不理解事物\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import DeletionAugmenter\n",
    "\n",
    "augmenter = DeletionAugmenter()\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EasyDataAugmenter\n",
    "\n",
    "这种一般都是来自论文的组合操作, 包含:\n",
    "\n",
    "- WordNet synonym replacement\n",
    "    - Randomly replace words with their synonyms.\n",
    "- Word deletion\n",
    "    - Randomly remove words from the sentence.\n",
    "- Word order swaps\n",
    "    - Randomly swap the position of words in the sentence.\n",
    "- Random synonym insertion\n",
    "    -Insert a random synonym of a random word at a random location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zhenh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不能创造我解理不的事物\n",
      "False\n",
      "我不能创造我不理解的事物\n",
      "True\n",
      "我不能创造我不理解的物\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import EasyDataAugmenter\n",
    "\n",
    "augmenter = EasyDataAugmenter()\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不能创造我不理解的事物\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import EmbeddingAugmenter\n",
    "\n",
    "# 这个默认的应该是英文的\n",
    "augmenter = EmbeddingAugmenter()\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "duplicate word '' in word2vec file, ignoring all but first\n",
      "duplicate word '' in word2vec file, ignoring all but first\n",
      "duplicate word '' in word2vec file, ignoring all but first\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embedding_path = r\"G:\\dataset\\词向量\\merge_sgns_bigram_char300.txt.bz2\"\n",
    "model = KeyedVectors.load_word2vec_format(embedding_path, binary=False, encoding=\"utf-8\", unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "duplicate word '' in word2vec file, ignoring all but first\n",
      "duplicate word '' in word2vec file, ignoring all but first\n",
      "duplicate word '' in word2vec file, ignoring all but first\n"
     ]
    }
   ],
   "source": [
    "# 好像没什么区别, 这个加载也太慢了\n",
    "embedding_path = r\"G:\\dataset\\词向量\\merge_sgns_bigram_char300.txt\"\n",
    "model = KeyedVectors.load_word2vec_format(embedding_path, binary=False, encoding=\"utf-8\", unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('祈晴祷雨', 0.5951941013336182),\n",
       " ('甘霖', 0.5938517451286316),\n",
       " ('雨泽', 0.5817687511444092),\n",
       " ('逢甘露', 0.56331467628479),\n",
       " ('霢', 0.5586028695106506),\n",
       " ('承雨露', 0.558268666267395),\n",
       " ('逢天', 0.5570719838142395),\n",
       " ('晴照', 0.5541519522666931),\n",
       " ('农禾', 0.5535107254981995),\n",
       " ('花满树', 0.5527628064155579)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"甘雨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爸妈不能创造我不理解的事物\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from textattack.shared import GensimWordEmbedding\n",
    "from textattack.transformations import WordSwapEmbedding\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "from textattack.transformations.sentence_transformations import BackTranslation\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.augmentation import Augmenter\n",
    "\n",
    "embedding = GensimWordEmbedding(model)\n",
    "transformation = WordSwapEmbedding(max_candidates=50, embedding=embedding)\n",
    "constraints = [RepeatModification(), StopwordModification()] + [WordEmbeddingDistance(min_cos_sim=0.8)]\n",
    "augmenter = Augmenter(transformation = transformation, constraints = constraints)\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwapAugmenter\n",
    "\n",
    "原理: 随机交换两个单词的顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不创能造我不理解的事物\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation.recipes import SwapAugmenter\n",
    "\n",
    "augmenter = SwapAugmenter()\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynonymInsertionAugmenter\n",
    "\n",
    "原理: 内部用的是 `wordnet.synsets`, 不支持中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不能创造我不理解的事物\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation.recipes import SynonymInsertionAugmenter\n",
    "\n",
    "augmenter = SynonymInsertionAugmenter()\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNetAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不能创造我不理解的事物\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zhenh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation.recipes import WordNetAugmenter\n",
    "\n",
    "augmenter = WordNetAugmenter()\n",
    "s = \"我不能创造我不理解的事物\"\n",
    "for item in augmenter.augment(s):\n",
    "    print(item)\n",
    "    print(item == s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
